{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_data(root_folder, user_id, model_name, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    folder_path = os.path.join(root_folder, f\"u{user_id.zfill(2)}\", model_name)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "            # Ensure the feature has consistent shape (adjust based on your data)\n",
    "            if feature.shape == ():\n",
    "                continue  # Skip empty features\n",
    "\n",
    "            # Flatten the feature to a one-dimensional array if necessary\n",
    "            feature = np.ravel(feature)\n",
    "\n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    return features_list, labels_list\n",
    "\n",
    "root_folder = \"D:/rp/dataset/ATVFS/features_new\"  # Change this to your actual root folder\n",
    "\n",
    "# Define models for both fake and original features\n",
    "fake_models = [\"fake_feature_densenet121\", \"fake_feature_efficientnetb0\", \"fake_feature_resnet50\",\"fake_feature_alexnet\", \"fake_feature_inceptionv3\", \"fake_feature_vgg16\"]\n",
    "original_models = [\"original_feature_densenet121\", \"original_feature_efficientnetb0\",  \"original_feature_resnet50\",\"original_feature_alexnet\", \"original_feature_inceptionv3\", \"original_feature_vgg16\"]\n",
    "\n",
    "# Load features and labels for fake data\n",
    "fake_features_list = []\n",
    "fake_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in fake_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=0)\n",
    "        fake_features_list.extend(features)\n",
    "        fake_labels_list.extend(labels)\n",
    "\n",
    "# Load features and labels for original data\n",
    "original_features_list = []\n",
    "original_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in original_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=1)\n",
    "        original_features_list.extend(features)\n",
    "        original_labels_list.extend(labels)\n",
    "\n",
    "# Find the maximum dimension for both fake and original features\n",
    "max_fake_dimension = max(feature.shape[0] for feature in fake_features_list)\n",
    "max_original_dimension = max(feature.shape[0] for feature in original_features_list)\n",
    "\n",
    "# Pad or reshape the features to have the same dimensions\n",
    "fake_features_list = [np.pad(feature, (0, max_fake_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_fake_dimension else feature for feature in fake_features_list]\n",
    "original_features_list = [np.pad(feature, (0, max_original_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_original_dimension else feature for feature in original_features_list]\n",
    "\n",
    "# Stack features along a new axis for both fake and original data\n",
    "all_fake_features = np.stack(fake_features_list, axis=1)  # Adjust axis if needed\n",
    "all_fake_labels = np.array(fake_labels_list)\n",
    "\n",
    "all_original_features = np.stack(original_features_list, axis=1)  # Adjust axis if needed\n",
    "all_original_labels = np.array(original_labels_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_data(root_folder, user_id, model_name, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    folder_path = os.path.join(root_folder, f\"u{user_id.zfill(2)}\", model_name)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "            # Ensure the feature has consistent shape (adjust based on your data)\n",
    "            if feature.shape == ():\n",
    "                continue  # Skip empty features\n",
    "\n",
    "            # Flatten the feature to a one-dimensional array if necessary\n",
    "            feature = np.ravel(feature)\n",
    "\n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    return features_list, labels_list\n",
    "\n",
    "root_folder = \"D:/rp/dataset/ATVFS/features_new\"  # Change this to your actual root folder\n",
    "\n",
    "# Define models for both fake and original features\n",
    "fake_models = [ \"fake_feature_resnet50\",\"fake_feature_vgg16\",\"fake_feature_alexnet\",\"fake_feature_inceptionv3\"]\n",
    "original_models = [ \"original_feature_resnet50\",\"original_feature_vgg16\",\"original_feature_alexnet\",\"original_feature_inceptionv3\"]\n",
    "\n",
    "# Load features and labels for fake data\n",
    "fake_features_list = []\n",
    "fake_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in fake_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=0)\n",
    "        fake_features_list.extend(features)\n",
    "        fake_labels_list.extend(labels)\n",
    "\n",
    "# Load features and labels for original data\n",
    "original_features_list = []\n",
    "original_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in original_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=1)\n",
    "        original_features_list.extend(features)\n",
    "        original_labels_list.extend(labels)\n",
    "\n",
    "# Find the maximum dimension for both fake and original features\n",
    "max_fake_dimension = max(feature.shape[0] for feature in fake_features_list)\n",
    "max_original_dimension = max(feature.shape[0] for feature in original_features_list)\n",
    "\n",
    "# Pad or reshape the features to have the same dimensions\n",
    "fake_features_list = [np.pad(feature, (0, max_fake_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_fake_dimension else feature for feature in fake_features_list]\n",
    "original_features_list = [np.pad(feature, (0, max_original_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_original_dimension else feature for feature in original_features_list]\n",
    "\n",
    "# Stack features along a new axis for both fake and original data\n",
    "all_fake_features_1 = np.stack(fake_features_list, axis=1)  # Adjust axis if needed\n",
    "all_fake_labels_1 = np.array(fake_labels_list)\n",
    "\n",
    "all_original_features_1 = np.stack(original_features_list, axis=1)  # Adjust axis if needed\n",
    "all_original_labels_1 = np.array(original_labels_list)\n",
    "\n",
    "# Use the stacked features and labels for further processing or classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2387867  1.6189588  0.         ... 0.         0.07823072 0.        ]\n",
      " [0.         0.35290718 0.         ... 0.         0.         0.17758268]\n",
      " [0.         0.         0.         ... 0.27954248 0.         0.62717474]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.16824819 0.        ]\n",
      " [0.         0.         0.         ... 0.08637016 0.20543297 0.        ]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.18813944 0.         ... 0.         1.2281507  0.        ]\n",
      " [0.         0.         0.         ... 0.         1.1306324  0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.1517345 ]\n",
      " [0.         0.         0.         ... 0.         0.89389545 0.        ]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "131072\n",
      "768\n",
      "131072\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(all_fake_features_1)\n",
    "print(all_fake_labels_1)\n",
    "print(all_original_features_1)\n",
    "print(all_original_labels_1)\n",
    "print(len(all_fake_features_1))\n",
    "print(len(all_fake_labels_1))\n",
    "print(len(all_original_features_1))\n",
    "print(len(all_original_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fake features and labels\n",
    "np.save(\"./RVAI/all_fake_features_1.npy\", all_fake_features_1)\n",
    "np.save(\"./RVAI/all_fake_labels_1.npy\", all_fake_labels_1)\n",
    "\n",
    "# Save original features and labels\n",
    "np.save(\"./RVAI/all_original_features_1.npy\", all_original_features_1)\n",
    "np.save(\"./RVAI/all_original_labels_1.npy\", all_original_labels_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired number of components to retain 95.0% variance: 240\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to fake features\n",
    "pca_fake = PCA()\n",
    "pca_fake.fit(all_fake_features_1)\n",
    "\n",
    "pca_original = PCA()\n",
    "pca_original.fit(all_original_features_1)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca_fake.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components to retain 95% of the variance\n",
    "desired_variance_ratio = 0.95\n",
    "desired_components = np.argmax(cumulative_explained_variance >= desired_variance_ratio) + 1\n",
    "\n",
    "print(f\"Desired number of components to retain {desired_variance_ratio * 100}% variance: {desired_components}\")\n",
    "\n",
    "# Apply PCA with the desired number of components\n",
    "pca_fake = PCA(n_components=50)\n",
    "pca_fake_features = pca_fake.fit_transform(all_fake_features_1)\n",
    "\n",
    "pca_original = PCA(n_components=50)\n",
    "pca_original_features = pca_fake.fit_transform(all_original_features_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100352\n",
      "100352\n",
      "Number of features after PCA: 50\n",
      "Number of features after PCA: 50\n"
     ]
    }
   ],
   "source": [
    "print(len(pca_fake_features))\n",
    "print(len(pca_original_features))\n",
    "print(f\"Number of features after PCA: {pca_fake_features.shape[1]}\")\n",
    "print(f\"Number of features after PCA: {pca_original_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./RVAI/all_fake_features_1_pca.npy\", pca_fake_features)\n",
    "\n",
    "np.save(\"./RVAI/all_original_features_1_pca.npy\", pca_original_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 90.91%\n",
      "Features shape: (1536, 768)\n",
      "Labels shape: (1536,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features and label\n",
    "fake_features = np.load(\"./RVAI/all_fake_features_1.npy\")\n",
    "original_features = np.load(\"./RVAI/all_original_features_1.npy\")\n",
    "\n",
    "fake_labels = np.load(\"./RVAI/all_fake_labels_1.npy\")\n",
    "original_labels = np.load(\"./RVAI/all_original_labels_1.npy\")\n",
    "\n",
    "# Find the minimum number of samples\n",
    "min_samples = min(fake_features.shape[0], original_features.shape[0], fake_labels.shape[0], original_labels.shape[0])\n",
    "\n",
    "# Extract the first 'min_samples' samples from both features and labels\n",
    "fake_features = fake_features[:min_samples]\n",
    "original_features = original_features[:min_samples]\n",
    "fake_labels = fake_labels[:min_samples]\n",
    "original_labels = original_labels[:min_samples]\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Now you can proceed to split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for SVM\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)  # Flatten the features\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_svm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "accuracy_svm_edm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm_edm * 100:.2f}%\")\n",
    "\n",
    "print(\"Features shape:\", all_features.shape)\n",
    "print(\"Labels shape:\", all_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 95.45%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "fake_features = np.load(\"./RVAI/all_fake_features_1_pca.npy\")\n",
    "original_features = np.load(\"./RVAI/all_original_features_1_pca.npy\")\n",
    "\n",
    "fake_labels = np.load(\"./RVAI/all_fake_labels_1.npy\")\n",
    "original_labels = np.load(\"./RVAI/all_original_labels_1.npy\")\n",
    "\n",
    "# Find the minimum number of samples\n",
    "min_samples = min(fake_features.shape[0], original_features.shape[0], fake_labels.shape[0], original_labels.shape[0])\n",
    "\n",
    "# Extract the first 'min_samples' samples from both features and labels\n",
    "fake_features = fake_features[:min_samples]\n",
    "original_features = original_features[:min_samples]\n",
    "fake_labels = fake_labels[:min_samples]\n",
    "original_labels = original_labels[:min_samples]\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Now you can proceed to split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for SVM\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)  # Flatten the features\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_svm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "accuracy_svm_pca_edm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm_pca_edm * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
