{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLYING PCA FOR INDIVIDUAL MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA applied and results saved to: D:/rp/dataset/ATVFS/feature_folder/u03/1_pca\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Input and output folder paths\n",
    "input_features_folder = \"D:/rp/dataset/ATVFS/feature_folder/u03/original_feature_efficientnetb0\"\n",
    "output_pca_folder = \"D:/rp/dataset/ATVFS/feature_folder/u03/1_pca\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_pca_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the extracted features folder\n",
    "for feature_filename in os.listdir(input_features_folder):\n",
    "    if feature_filename.endswith(\".npy\"):\n",
    "        feature_filepath = os.path.join(input_features_folder, feature_filename)\n",
    "\n",
    "        # Read feature information from the NumPy file\n",
    "        feature_data = np.load(feature_filepath, allow_pickle=True)\n",
    "\n",
    "        # Extract relevant information for creating KML (adjust this part based on your data)\n",
    "        features = feature_data[0][0]  # Assuming features are at index 0\n",
    "\n",
    "        # Apply PCA to reduce dimensionality\n",
    "        pca = PCA(n_components=5)  # Set the desired number of components\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "\n",
    "        # Save the reduced features to a new file in the output PCA folder\n",
    "        output_pca_filepath = os.path.join(output_pca_folder, feature_filename.replace(\".npy\", \"_pca.npy\"))\n",
    "        np.save(output_pca_filepath, reduced_features)\n",
    "\n",
    "print(f\"PCA applied and results saved to: {output_pca_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCATENATING FEATURE FODLERS AFTER APPLYING PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated PCA features saved to: ./concatenated_pca_features/fake and ./concatenated_pca_features/original\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Base folder path\n",
    "base_folder = \"D:/rp/dataset/ATVFS/feature_folder\"\n",
    "\n",
    "# List of user folders (u01, u02, u03, u04)\n",
    "user_folders = [\"u01\", \"u02\", \"u03\", \"u04\"]\n",
    "\n",
    "# List of PCA folders (1_pca, 2_pca, 3_pca, 4_pca)\n",
    "pca_folders = [\"1_pca\", \"2_pca\", \"3_pca\", \"4_pca\"]\n",
    "\n",
    "# Output folder for concatenated PCA features\n",
    "output_folder_fake = \"./concatenated_pca_features/fake\"\n",
    "output_folder_original = \"./concatenated_pca_features/original\"\n",
    "\n",
    "# Create the output folders if they don't exist\n",
    "os.makedirs(output_folder_fake, exist_ok=True)\n",
    "os.makedirs(output_folder_original, exist_ok=True)\n",
    "\n",
    "# Iterate through user folders\n",
    "for user_folder in user_folders:\n",
    "    concatenated_fake_features = None\n",
    "    concatenated_original_features = None\n",
    "\n",
    "    # Iterate through fake and original subfolders\n",
    "    for data_type in [\"fake\", \"original\"]:\n",
    "        # Iterate through PCA folders\n",
    "        for pca_folder in pca_folders:\n",
    "            # Construct the path to the PCA folder\n",
    "            pca_folder_path = os.path.join(base_folder, user_folder, data_type, pca_folder)\n",
    "\n",
    "            # Collect all .npy files in the PCA folder\n",
    "            pca_files = [file for file in os.listdir(pca_folder_path) if file.endswith(\".npy\")]\n",
    "\n",
    "            # Concatenate the features from all .npy files\n",
    "            for pca_file in pca_files:\n",
    "                pca_filepath = os.path.join(pca_folder_path, pca_file)\n",
    "                pca_features = np.load(pca_filepath, allow_pickle=True)\n",
    "\n",
    "                # Concatenate the features based on data type\n",
    "                if data_type == \"fake\":\n",
    "                    if concatenated_fake_features is None:\n",
    "                        concatenated_fake_features = pca_features\n",
    "                    else:\n",
    "                        concatenated_fake_features = np.concatenate((concatenated_fake_features, pca_features), axis=1)\n",
    "                elif data_type == \"original\":\n",
    "                    if concatenated_original_features is None:\n",
    "                        concatenated_original_features = pca_features\n",
    "                    else:\n",
    "                        concatenated_original_features = np.concatenate((concatenated_original_features, pca_features), axis=1)\n",
    "\n",
    "    # Save the concatenated features to new files in the output folders\n",
    "    output_fake_filepath = os.path.join(output_folder_fake, f\"{user_folder}_concatenated_fake_pca.npy\")\n",
    "    output_original_filepath = os.path.join(output_folder_original, f\"{user_folder}_concatenated_original_pca.npy\")\n",
    "\n",
    "    np.save(output_fake_filepath, concatenated_fake_features)\n",
    "    np.save(output_original_filepath, concatenated_original_features)\n",
    "\n",
    "print(f\"Concatenated PCA features saved to: {output_folder_fake} and {output_folder_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.0787 - accuracy: 0.5000 - val_loss: 93.3889 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 80.5512 - accuracy: 0.5000 - val_loss: 32.6810 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6822e-07 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 47.6863 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 89.3264 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 121.9194 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 9.4465e-04 - accuracy: 1.0000 - val_loss: 141.7504 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 4.1939 - accuracy: 0.7500 - val_loss: 116.9154 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 89.1268 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 62.9854 - val_accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 18.1946 - accuracy: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "def load_data(folder_path, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "            if feature.shape == ():\n",
    "                continue  \n",
    "            feature = np.ravel(feature)\n",
    "            \n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    return features_list, labels_list\n",
    "\n",
    "fake_folder_path = \"./concatenated_pca_features/fake/\"\n",
    "original_folder_path = \"./concatenated_pca_features/original/\"\n",
    "\n",
    "fake_features, fake_labels = load_data(fake_folder_path, label=0)\n",
    "original_features, original_labels = load_data(original_folder_path, label=1)\n",
    "\n",
    "max_length = max(len(feature) for feature in fake_features + original_features)\n",
    "fake_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in fake_features]\n",
    "original_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in original_features]\n",
    "\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels_encoded = label_encoder.fit_transform(all_labels)\n",
    "all_labels_categorical = to_categorical(all_labels_encoded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(max_length, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load features and labels\n",
    "def load_data(folder_path, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "            \n",
    "            # Ensure the feature has consistent shape (adjust based on your data)\n",
    "            if feature.shape == ():\n",
    "                continue  # Skip empty features\n",
    "            \n",
    "            # Flatten the feature to a one-dimensional array\n",
    "            feature = np.ravel(feature)\n",
    "            \n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    return features_list, labels_list\n",
    "\n",
    "# Folder paths for fake and original features\n",
    "fake_folder_path = \"./concatenated_pca_features/fake/\"\n",
    "original_folder_path = \"./concatenated_pca_features/original/\"\n",
    "\n",
    "# Load fake and original features and labels\n",
    "fake_features, fake_labels = load_data(fake_folder_path, label=0)\n",
    "original_features, original_labels = load_data(original_folder_path, label=1)\n",
    "\n",
    "# Ensure consistent feature dimensions\n",
    "max_length = max(len(feature) for feature in fake_features + original_features)\n",
    "fake_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in fake_features]\n",
    "original_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in original_features]\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels_encoded = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for SVM\n",
    "X_train_svm = np.vstack(X_train)\n",
    "X_test_svm = np.vstack(X_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_svm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load features and labels\n",
    "def load_data(folder_path, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "            \n",
    "            # Ensure the feature has consistent shape (adjust based on your data)\n",
    "            if feature.shape == ():\n",
    "                continue  # Skip empty features\n",
    "            \n",
    "            # Flatten the feature to a one-dimensional array\n",
    "            feature = np.ravel(feature)\n",
    "            \n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    return features_list, labels_list\n",
    "\n",
    "# Folder paths for fake and original features\n",
    "fake_folder_path = \"./concatenated_pca_features/fake/\"\n",
    "original_folder_path = \"./concatenated_pca_features/original/\"\n",
    "\n",
    "# Load fake and original features and labels\n",
    "fake_features, fake_labels = load_data(fake_folder_path, label=0)\n",
    "original_features, original_labels = load_data(original_folder_path, label=1)\n",
    "\n",
    "# Ensure consistent feature dimensions\n",
    "max_length = max(len(feature) for feature in fake_features + original_features)\n",
    "fake_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in fake_features]\n",
    "original_features = [np.pad(feature, (0, max_length - len(feature)), 'constant') for feature in original_features]\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels_encoded = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for logistic regression\n",
    "X_train_lr = np.vstack(X_train)\n",
    "X_test_lr = np.vstack(X_test)\n",
    "\n",
    "# Train the logistic regression classifier\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_lr, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_classifier.predict(X_test_lr)\n",
    "\n",
    "# Evaluate the logistic regression model\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
