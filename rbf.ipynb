{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_data(root_folder, user_id, model_name, label):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    folder_path = os.path.join(root_folder, f\"u{user_id.zfill(2)}\", model_name)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            feature = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "            # Ensure the feature has consistent shape (adjust based on your data)\n",
    "            if feature.shape == ():\n",
    "                continue  # Skip empty features\n",
    "\n",
    "            # Flatten the feature to a one-dimensional array if necessary\n",
    "            feature = np.ravel(feature)\n",
    "\n",
    "            features_list.append(feature)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    return features_list, labels_list\n",
    "\n",
    "root_folder = \"D:/rp/dataset/ATVFS/features_new\"  \n",
    "\n",
    "fake_models = [\"fake_feature_densenet121\", \"fake_feature_efficientnetb0\", \"fake_feature_resnet50\",\"fake_feature_alexnet\",\n",
    "                \"fake_feature_inceptionv3\", \"fake_feature_vgg16\"]\n",
    "original_models = [\"original_feature_densenet121\", \"original_feature_efficientnetb0\",  \"original_feature_resnet50\",\n",
    "                   \"original_feature_alexnet\", \"original_feature_inceptionv3\", \"original_feature_vgg16\"]\n",
    "\n",
    "# Load features and labels for fake data\n",
    "fake_features_list = []\n",
    "fake_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in fake_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=0)\n",
    "        fake_features_list.extend(features)\n",
    "        fake_labels_list.extend(labels)\n",
    "\n",
    "# Load features and labels for original data\n",
    "original_features_list = []\n",
    "original_labels_list = []\n",
    "\n",
    "for user_id in range(1, 5):  # Assuming user IDs u01 to u04\n",
    "    for model_name in original_models:\n",
    "        features, labels = load_data(root_folder, str(user_id).zfill(2), model_name, label=1)\n",
    "        original_features_list.extend(features)\n",
    "        original_labels_list.extend(labels)\n",
    "\n",
    "# Find the maximum dimension for both fake and original features\n",
    "max_fake_dimension = max(feature.shape[0] for feature in fake_features_list)\n",
    "max_original_dimension = max(feature.shape[0] for feature in original_features_list)\n",
    "\n",
    "# Pad or reshape the features to have the same dimensions\n",
    "fake_features_list = [np.pad(feature, (0, max_fake_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_fake_dimension else feature for feature in fake_features_list]\n",
    "original_features_list = [np.pad(feature, (0, max_original_dimension - feature.shape[0]), 'constant') if feature.shape[0] < max_original_dimension else feature for feature in original_features_list]\n",
    "\n",
    "# Stack features along a new axis for both fake and original data\n",
    "all_fake_features = np.stack(fake_features_list, axis=1)  # Adjust axis if needed\n",
    "all_fake_labels = np.array(fake_labels_list)\n",
    "\n",
    "all_original_features = np.stack(original_features_list, axis=1)  # Adjust axis if needed\n",
    "all_original_labels = np.array(original_labels_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "gamma = 1.0 / all_fake_features.shape[0]  \n",
    "gamma = 1.0 / all_original_features.shape[0]\n",
    "\n",
    "all_fake_features = rbf_kernel(all_fake_features.T, gamma=gamma)\n",
    "\n",
    "all_original_features = rbf_kernel(all_original_features.T, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fake features and labels\n",
    "np.save(\"./all_fake_features_rbf.npy\", all_fake_features)\n",
    "np.save(\"./all_fake_labels.npy\", all_fake_labels)\n",
    "\n",
    "# Save original features and labels\n",
    "np.save(\"./all_original_features_rbf.npy\", all_original_features)\n",
    "np.save(\"./all_original_labels.npy\", all_original_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 7.0014352e-01 7.0671576e-01 ... 4.6463036e-05\n",
      "  4.1350449e-06 2.7574189e-03]\n",
      " [7.0014352e-01 1.0000000e+00 6.4843917e-01 ... 4.8052181e-05\n",
      "  4.1259300e-06 2.8378163e-03]\n",
      " [7.0671576e-01 6.4843917e-01 1.0000000e+00 ... 4.7974750e-05\n",
      "  4.2467759e-06 2.8834499e-03]\n",
      " ...\n",
      " [4.6463036e-05 4.8052181e-05 4.7974750e-05 ... 1.0000000e+00\n",
      "  1.3993655e-05 1.0063327e-05]\n",
      " [4.1350449e-06 4.1259300e-06 4.2467759e-06 ... 1.3993655e-05\n",
      "  1.0000000e+00 9.0996423e-07]\n",
      " [2.7574189e-03 2.8378163e-03 2.8834499e-03 ... 1.0063327e-05\n",
      "  9.0996423e-07 1.0000000e+00]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1.0000000e+00 7.8205645e-01 7.8597564e-01 ... 4.7976174e-04\n",
      "  1.2182771e-03 2.2377873e-04]\n",
      " [7.8205645e-01 1.0000000e+00 8.0275476e-01 ... 4.4002640e-04\n",
      "  1.1147797e-03 2.0641035e-04]\n",
      " [7.8597564e-01 8.0275476e-01 1.0000000e+00 ... 4.2376795e-04\n",
      "  1.0552508e-03 1.9668184e-04]\n",
      " ...\n",
      " [4.7976174e-04 4.4002640e-04 4.2376795e-04 ... 1.0000000e+00\n",
      "  1.4918220e-03 2.8085927e-02]\n",
      " [1.2182771e-03 1.1147797e-03 1.0552508e-03 ... 1.4918220e-03\n",
      "  1.0000000e+00 1.9238378e-03]\n",
      " [2.2377873e-04 2.0641035e-04 1.9668184e-04 ... 2.8085927e-02\n",
      "  1.9238378e-03 1.0000000e+00]]\n",
      "[1 1 1 ... 1 1 1]\n",
      "1152\n",
      "1152\n",
      "1152\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "print(all_fake_features)\n",
    "print(all_fake_labels)\n",
    "print(all_original_features)\n",
    "print(all_original_labels)\n",
    "print(len(all_fake_features))\n",
    "print(len(all_fake_labels))\n",
    "print(len(all_original_features))\n",
    "print(len(all_original_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired number of components to retain 95.0% variance: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_fake = PCA()\n",
    "pca_fake.fit(all_fake_features)\n",
    "\n",
    "pca_original = PCA()\n",
    "pca_original.fit(all_original_features)\n",
    "\n",
    "cumulative_explained_variance = np.cumsum(pca_fake.explained_variance_ratio_)\n",
    "\n",
    "desired_variance_ratio = 0.95\n",
    "desired_components = np.argmax(cumulative_explained_variance >= desired_variance_ratio) + 1\n",
    "\n",
    "print(f\"Desired number of components to retain {desired_variance_ratio * 100}% variance: {desired_components}\")\n",
    "\n",
    "pca_fake = PCA(n_components=desired_components)\n",
    "pca_fake_features = pca_fake.fit_transform(all_fake_features)\n",
    "\n",
    "pca_original = PCA(n_components=desired_components)\n",
    "pca_original_features = pca_fake.fit_transform(all_original_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 7.0014352e-01 7.0671576e-01 ... 4.6463036e-05\n",
      "  4.1350449e-06 2.7574189e-03]\n",
      " [7.0014352e-01 1.0000000e+00 6.4843917e-01 ... 4.8052181e-05\n",
      "  4.1259300e-06 2.8378163e-03]\n",
      " [7.0671576e-01 6.4843917e-01 1.0000000e+00 ... 4.7974750e-05\n",
      "  4.2467759e-06 2.8834499e-03]\n",
      " ...\n",
      " [4.6463036e-05 4.8052181e-05 4.7974750e-05 ... 1.0000000e+00\n",
      "  1.3993655e-05 1.0063327e-05]\n",
      " [4.1350449e-06 4.1259300e-06 4.2467759e-06 ... 1.3993655e-05\n",
      "  1.0000000e+00 9.0996423e-07]\n",
      " [2.7574189e-03 2.8378163e-03 2.8834499e-03 ... 1.0063327e-05\n",
      "  9.0996423e-07 1.0000000e+00]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1.0000000e+00 7.8205645e-01 7.8597564e-01 ... 4.7976174e-04\n",
      "  1.2182771e-03 2.2377873e-04]\n",
      " [7.8205645e-01 1.0000000e+00 8.0275476e-01 ... 4.4002640e-04\n",
      "  1.1147797e-03 2.0641035e-04]\n",
      " [7.8597564e-01 8.0275476e-01 1.0000000e+00 ... 4.2376795e-04\n",
      "  1.0552508e-03 1.9668184e-04]\n",
      " ...\n",
      " [4.7976174e-04 4.4002640e-04 4.2376795e-04 ... 1.0000000e+00\n",
      "  1.4918220e-03 2.8085927e-02]\n",
      " [1.2182771e-03 1.1147797e-03 1.0552508e-03 ... 1.4918220e-03\n",
      "  1.0000000e+00 1.9238378e-03]\n",
      " [2.2377873e-04 2.0641035e-04 1.9668184e-04 ... 2.8085927e-02\n",
      "  1.9238378e-03 1.0000000e+00]]\n",
      "[1 1 1 ... 1 1 1]\n",
      "1152\n",
      "1152\n",
      "1152\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "print(all_fake_features)\n",
    "print(all_fake_labels)\n",
    "print(all_original_features)\n",
    "print(all_original_labels)\n",
    "print(len(all_fake_features))\n",
    "print(len(all_fake_labels))\n",
    "print(len(all_original_features))\n",
    "print(len(all_original_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fake features and labels\n",
    "np.save(\"./all_fake_features.npy\", all_fake_features)\n",
    "np.save(\"./all_fake_labels.npy\", all_fake_labels)\n",
    "\n",
    "# Save original features and labels\n",
    "np.save(\"./all_original_features.npy\", all_original_features)\n",
    "np.save(\"./all_original_labels.npy\", all_original_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./all_fake_features_face_pca.npy\", pca_fake_features)\n",
    "\n",
    "np.save(\"./all_original_features_face_pca.npy\", pca_original_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 80.69%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "fake_features = np.load(\"./all_fake_features.npy\")\n",
    "original_features = np.load(\"./all_original_features.npy\")\n",
    "\n",
    "fake_labels = np.load(\"./all_fake_labels.npy\")\n",
    "original_labels = np.load(\"./all_original_labels.npy\")\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Reshape features to match the number of labels\n",
    "all_features_reshaped = all_features[:2304, :]\n",
    "\n",
    "# Now you can proceed to split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features_reshaped, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for SVM\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)  # Flatten the features\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_svm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "accuracy_svm_4 = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm_4 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 48.37%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "fake_features = np.load(\"./all_fake_features_pca.npy\")\n",
    "original_features = np.load(\"./all_original_features_pca.npy\")\n",
    "\n",
    "fake_labels = np.load(\"./all_fake_labels.npy\")\n",
    "original_labels = np.load(\"./all_original_labels.npy\")\n",
    "\n",
    "# Combine features and labels\n",
    "all_features = np.concatenate((fake_features, original_features), axis=0)\n",
    "all_labels = np.concatenate((fake_labels, original_labels), axis=0)\n",
    "\n",
    "# Reshape features to match the number of labels\n",
    "all_features_reshaped = all_features[:2304, :]\n",
    "\n",
    "# Now you can proceed to split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features_reshaped, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape features for SVM\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)  # Flatten the features\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_svm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "accuracy_svm_pca_4 = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm_pca_4 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
